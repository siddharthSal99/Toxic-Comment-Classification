{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "from math import log\n",
    "import math\n",
    "import numbers\n",
    "import csv\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "In this section, you will implement the equations necessary to pick the best attribute based on information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "def entropy(class_y):\n",
    "    # Input:            \n",
    "    #   class_y         : list of class labels (0's and 1's)\n",
    "    \n",
    "    # TODO: Compute the entropy (in log base 2) for a list of classes\n",
    "    \n",
    "    \"\"\"\n",
    "    Example:\n",
    "       entropy([1,1,0,1,0,0,1,0,1]) = 0.92\n",
    "    \"\"\"\n",
    "    num_points = len(class_y)\n",
    "    if num_points == 0:\n",
    "        return 0\n",
    "    pos_vals = sum(class_y)\n",
    "    neg_vals = num_points - pos_vals\n",
    "    \n",
    "    pos_prod = 0 if pos_vals == 0 else (float(pos_vals) / num_points) * (math.log((float(pos_vals) / num_points), 2))\n",
    "    neg_prod = 0 if neg_vals == 0 else (float(neg_vals) / num_points) * (math.log((float(neg_vals) / num_points), 2))                                      \n",
    "    \n",
    "    return -(pos_prod + neg_prod)\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044110417748401076"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def information_gain(y_before_split, y_after_split):\n",
    "    # Inputs:\n",
    "    #   y_before_split: the y labels before splitting (0's and 1's)\n",
    "    #   y_after_split:  the y labels after splitting in two. This is a list of lists.\n",
    "    #\n",
    "    # TODO: Compute the information gain given this split.\n",
    "    \n",
    "    \"\"\"\n",
    "    Example:\n",
    "    \n",
    "    previous_y = [0,0,0,1,1,1]\n",
    "    current_y = [[0,0], [1,1,1,0]]\n",
    "    \n",
    "    info_gain = 0.45915\n",
    "    \"\"\"\n",
    "    H_parent = entropy(y_before_split)\n",
    "    H_rightChild = entropy(y_after_split[1])\n",
    "    H_leftChild = entropy(y_after_split[0])\n",
    "    \n",
    "    num_points = len(y_before_split)\n",
    "    r_weight = len(y_after_split[1]) / float(num_points)\n",
    "    l_weight = len(y_after_split[0]) / float(num_points)\n",
    "    \n",
    "    gain = H_parent - (r_weight * H_rightChild + l_weight * H_leftChild)\n",
    "    return gain\n",
    "\n",
    "previous_y = [0,0,1,1,1,1]\n",
    "current_y = [[0,1], [1,1,1,0]]\n",
    "    \n",
    "information_gain(previous_y,current_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "\n",
    "This section implements a utility function that splits data (X,y) based on an attribute and its value. This will be used in the learning procedure of your decision tree when \"filtering\" data through a decision node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['2', 'cc', '2'], ['4', 'cc', '3']],\n",
       " [0, 1],\n",
       " [['3', 'aa', '0'], ['1', 'bb', '3'], ['5', 'bb', '0']],\n",
       " [1, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def partition_classes(X, y, split_attribute_idx, split_val):\n",
    "    # Inputs:\n",
    "    #   X                   : data containing all attributes\n",
    "    #   y                   : labels\n",
    "    #   split_attribute_idx : column index (in X) of the attribute to split on\n",
    "    #   split_val           : categorical value to divide the split_attribute\n",
    "    \n",
    "    # TODO: Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "    #.      \n",
    "    #\n",
    "    # You can perform the partition in the following way\n",
    "    #   Split the data X into two lists(X_left and X_right) where the first list has all \n",
    "    #   the rows where the split attribute is equal to the split value, and the second list\n",
    "    #   has all the rows where the split attribute is not equal to the split value.\n",
    "    #   Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    "    '''\n",
    "    Example:\n",
    "    \n",
    "    X = [[3, 'aa', 0],                 y = [1,\n",
    "         [1, 'bb', 3],                      1,\n",
    "         [2, 'cc', 2],                      0,\n",
    "         [5, 'bb', 0],                      0,\n",
    "         [4, 'cc', 3]]                      1]\n",
    "    \n",
    "    Here, columns 0 and 2 represent numeric attributes, while column 1 is a categorical attribute.\n",
    "\n",
    "    Consider the case where we call the function with split_attribute = 1 and split_val = 'bb'\n",
    "    Then we divide X into two lists, one where column 1 is 'bb', and the other where it is not 'bb'.\n",
    "        \n",
    "    X_left = [[1, 'bb', 3],                 y_left = [1,\n",
    "              [5, 'bb', 0]]                           0]\n",
    "              \n",
    "    X_right = [[3, 'aa', 0],                y_right = [1,\n",
    "               [2, 'cc', 2],                           0,\n",
    "               [4, 'cc', 3]]                           1]\n",
    "               \n",
    "    ''' \n",
    "    X_orig = np.array(X)\n",
    "    y_orig = np.array(y)\n",
    "\n",
    "    if len(np.shape(X_orig)) == 1:\n",
    "        if X[split_attribute_idx] == split_val:\n",
    "            X_left = X_orig\n",
    "            y_left = y_orig\n",
    "            X_right = np.array()\n",
    "            y_right = np.array()\n",
    "        else:\n",
    "            X_right = X_orig\n",
    "            y_right = y_orig\n",
    "            X_left = np.array()\n",
    "            y_left = np.array()\n",
    "        return (X_left, y_left, X_right, y_right)\n",
    "    else:\n",
    "        split_column = X_orig[:, split_attribute_idx] #returns a row vector\n",
    "        mask_equals = (split_column == split_val) \n",
    "        mask_notEquals = (split_column != split_val)\n",
    "\n",
    "        \n",
    "        X_left = X_orig[mask_equals, :]\n",
    "        X_right = X_orig[mask_notEquals, :]\n",
    "    \n",
    "    \n",
    "        y_left = y_orig[mask_equals]\n",
    "        y_right = y_orig[mask_notEquals]\n",
    "    \n",
    "        X_left = X_left.tolist()\n",
    "        X_right = X_right.tolist()\n",
    "        y_left = y_left.tolist()\n",
    "        y_right = y_right.tolist()\n",
    "    \n",
    "        return (X_left, y_left, X_right, y_right)\n",
    "\n",
    "X = [[3, 'aa', 0],                 \n",
    "     [1, 'bb', 3],                  \n",
    "     [2, 'cc', 2],                      \n",
    "     [5, 'bb', 0],                      \n",
    "     [4, 'cc', 3]]                  \n",
    "y = [1,1,0,0,1]\n",
    "partition_classes(X,y,1,\"cc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "This section implements a decision tree class that can be used for learning and predicting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, minSplitPercent=5, maxDepth=100):\n",
    "        # Inputs:\n",
    "        #   minSplitPercent : minimum percent required for a node to be split \n",
    "        #   maxDepth        : maximum allowed depth of a tree.\n",
    "        \n",
    "        self.tree = {'depth' : 0}                   # create root node, and set depth parameter to 0\n",
    "        self.minSplitPercent = minSplitPercent;     \n",
    "        self.maxDepth = maxDepth;\n",
    "    \n",
    "    \n",
    "    def shouldStopSplitting(self, y, depth):\n",
    "        # Inputs\n",
    "        #   y     : a list of the y labels (0's and 1's)\n",
    "        #   depth : the depth of the current node\n",
    "        #\n",
    "        # This method returns True if the maximum depth has been reached, or if \n",
    "        # the percentage of 0's or 1's in y is below self.minSplitPercent (see __init__)\n",
    "        # Otherwise, return False.\n",
    "        #\n",
    "        # TODO: Your code here\n",
    "        a = depth >= self.maxDepth\n",
    "        if len(y) == 0:\n",
    "            return True\n",
    "        \n",
    "        num_labels = len(y)\n",
    "        num_pos = sum(y)\n",
    "        p_y1 = (num_pos / float(num_labels)) * 100\n",
    "        p_y0 = 100 - p_y1\n",
    "        \n",
    "        b = (p_y1 < self.minSplitPercent) | (p_y0 < self.minSplitPercent)\n",
    "        return a | b\n",
    "    \n",
    "    def terminate(self, node, y):\n",
    "        #converts the node in question into a leaf by changing its 'leaf' status to true\n",
    "        #it also assigns it a class based on the y data\n",
    "        #Inputs:\n",
    "        # node : The node that is a leaf\n",
    "        # y    : The labels that will determine if the class is 1 or 0\n",
    "        if (sum(y) / float(len(y))) >= 0.5:\n",
    "            node['class'] = 1\n",
    "            node['leaf'] = True\n",
    "        else:\n",
    "            node['class'] = 0\n",
    "            node['leaf'] = True\n",
    "            \n",
    "    \n",
    "    def learn(self, X, y):\n",
    "        # call recursive_learn on the root node\n",
    "        self.recursive_learn(X, y, self.tree);\n",
    "        \n",
    "    def recursive_learn(self, X, y, node):\n",
    "        # Inputs:\n",
    "        #   X    : data containing all attributes. This X only contains the data for the current node.\n",
    "        #   y    : labels of datapoints for the current node.\n",
    "        #   node : current node we're working with. A dictionary that has keys such as 'depth'. Example \n",
    "        #          node structures that you can use are:\n",
    "        #          - For a leaf at depth 2 that outputs class 1: {'depth' : 2, 'leaf' : True, 'class' : 1}  \n",
    "        #          - For a non-leaf node at depth 4: {'depth' : 4, 'leaf' : False, 'left' : {}, 'right' : {}}\n",
    "        #            where 'left' and 'right' keys will be the left and right children, which are themselves \n",
    "        #            dictionaries.\n",
    "        #\n",
    "        # This method decides whether the current node should be split further or set as a leaf (with the appropriate\n",
    "        # class);\n",
    "        #\n",
    "        # Recommended code structure (not required though)\n",
    "        # 1) Check if 'node' should be split. If not, then set it as leaf and set it's class.\n",
    "        # 2) Otherwise, find the best attribute to split on.\n",
    "        #    - Note that since the the categorical variables here have non-binary values (e.g. 0,1,2,...k)\n",
    "        #      and given that we're working with a binary tree, you'll have to find the best attribute value\n",
    "        #      to split on for each attribute. \n",
    "        # 3) Once the best attribute is found, create two descendent nodes (left and right), set their attributes \n",
    "        #    (e.g. depth), and split the data (X and y) based on the attribute and value selected above. \n",
    "        #    You should use the partition_classes function implemented earlier.\n",
    "        # 4) Call the \"recursive_learn\" function on each of the descendents. E.g.\n",
    "        #    self.recursive_learn(X_left,  y_left,  node[\"left\"])\n",
    "        #    self.recursive_learn(X_right, y_right, node[\"right\"])\n",
    "        #\n",
    "        # TODO: Your code here.\n",
    "        # Note: This method does not return anything. It just builds the tree through self.tree\n",
    "        X = np.array(X)\n",
    "        if self.shouldStopSplitting(y , node['depth']):\n",
    "            self.terminate(node, y)\n",
    "        else: \n",
    "            node['leaf'] = False\n",
    "            dims = np.shape(X)\n",
    "            infogain_curr = 0\n",
    "            final_splitVal = 0 \n",
    "            final_idx = 0\n",
    "            \n",
    "            for i in range(0, dims[1]):\n",
    "                feature_n = X[:, i]\n",
    "                unique_f = np.unique(feature_n)\n",
    "                for splitVal in unique_f:\n",
    "                    splitClass = partition_classes(X,y,i,splitVal)\n",
    "                    ig = information_gain(y,[splitClass[1], splitClass[3]])\n",
    "                    if ig >= infogain_curr:\n",
    "                        infogain_curr = ig\n",
    "                        final_splitVal = splitVal\n",
    "                        final_idx = i\n",
    "            final_split = partition_classes(X, y, final_idx, final_splitVal)\n",
    "            left = {'depth' : node['depth'] + 1}\n",
    "            right = {'depth' : node['depth'] + 1}\n",
    "            node['left'] = left\n",
    "            node['right'] = right\n",
    "            node['splitVal'] = final_splitVal\n",
    "            node['split_idx'] = final_idx\n",
    "            self.recursive_learn(final_split[0], final_split[1], node['left'])\n",
    "            self.recursive_learn(final_split[2], final_split[3], node['right'])\n",
    "            \n",
    "    def classify(self, record):\n",
    "        # Inputs:\n",
    "        #   record : a new, single x data point \n",
    "        #\n",
    "        # TODO: using the tree (self.tree), determine and return the class (0 or 1) of this data point.\n",
    "        return self.rec_classify(record, self.tree)\n",
    "    \n",
    "    def rec_classify(self, record, node):\n",
    "        \n",
    "        if node['leaf']:\n",
    "            return node['class']\n",
    "        else:\n",
    "            if record[node['split_idx']] == node['splitVal']:\n",
    "                return self.rec_classify(record, node['left'])\n",
    "            else:\n",
    "                return self.rec_classify(record, node['right'])\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing your code\n",
    "\n",
    "Below is a function that tests your decision tree by loading a Mushroom dataset (edible vs poisonous) and computes the training accuracy of the decision tree. If everything above is done correctly, you should get an accuracy of 0.9852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n",
      "training accuracy: 0.9852\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X = list()\n",
    "    y = list()\n",
    "    numerical_cols = set([]) # indices of numeric attributes (columns)\n",
    "\n",
    "    # Loading data set\n",
    "    print 'reading data'\n",
    "    with open(\"agaricus-lepiota.csv\") as f:\n",
    "        next(f, None)\n",
    "\n",
    "        for line in csv.reader(f, delimiter=\",\"):\n",
    "            xline = []\n",
    "            for i in range(len(line)):\n",
    "                xline.append(line[i])\n",
    "\n",
    "            X.append(xline[:-1])\n",
    "            y.append(int(xline[-1]))\n",
    "    \n",
    "    d = DecisionTree();\n",
    "    d.learn(X, y);\n",
    "    \n",
    "    y_predicted = [d.classify(record) for record in X];\n",
    "\n",
    "    # Comparing predicted and true labels\n",
    "    results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True)) / float(len(results))\n",
    "\n",
    "    print \"training accuracy: %.4f\" % accuracy\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
